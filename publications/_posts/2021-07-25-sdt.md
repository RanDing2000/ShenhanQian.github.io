---
layout: publication
title:  "Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates"
authors: ["Shenhan Qian*", "Zhi Tu*", "Yihao Zhi*", "Wen Liu", "Shenghua Gao"]
publisher: International Conference on Computer Vision (ICCV), 2021
description: Generating upper-body gestures from voice with decoupled gesture styles and voice-compatible movements.
paper: https://openaccess.thecvf.com/content/ICCV2021/papers/Qian_Speech_Drives_Templates_Co-Speech_Gesture_Synthesis_With_Learned_Templates_ICCV_2021_paper.pdf
arxiv: https://arxiv.org/abs/2108.08020
poster: /assets/2021-07-25-sdt/poster.pdf
video: https://youtu.be/yu-5gUHn6h8
code: https://github.com/ShenhanQian/SpeechDrivesTemplates
teaser_image: /assets/2021-07-25-sdt/cover.jpg
teaser_video: /assets/2021-07-25-sdt/cover.mp4
project: sdt
permalink: sdt
---


![pipeline](/assets/2021-07-25-sdt/pipeline.jpg)
## Abstract

Co-speech gesture generation is to synthesize a gesture sequence that not only looks real but also matches with the input speech audio. Our method generates the movements of a complete upper body, including arms, hands, and the head. Although recent data-driven methods achieve great success, challenges still exist, such as limited variety, poor fidelity, and lack of objective metrics. Motivated by the fact that the speech cannot fully determine the gesture, we design a method that learns a set of gesture template vectors to model the latent conditions, which relieve the ambiguity. For our method, the template vector determines the general appearance of a generated gesture sequence, while the speech audio drives subtle movements of the body, both indispensable for synthesizing a realistic gesture sequence. Due to the intractability of an objective metric for gesture-speech synchronization, we adopt the lip-sync error as a proxy metric to tune and evaluate the synchronization ability of our model. Extensive experiments show the superiority of our method in both objective and subjective evaluations on fidelity and synchronization.

## Template vs. Speech

### Different templates driven by the same speech clip
(Unmute the following videos for the speech audio.)
<video autoplay loop muted controls width="100%">
  <source src="/assets/2021-07-25-sdt/varying-template.mp4" type="video/mp4">
</video>

### The same template driven by different speech clips
<video autoplay loop muted controls width="100%">
  <source src="/assets/2021-07-25-sdt/varying-speech.mp4" type="video/mp4">
</video>

### Visualization of learned template gestures
<video autoplay loop muted width="100%">
  <source src="/assets/2021-07-25-sdt/template-gestures.mp4" type="video/mp4">
</video>

## Comparisons
### Oliver
<video autoplay loop muted controls width="100%">
  <source src="/assets/2021-07-25-sdt/comparison-oliver.mp4" type="video/mp4">
</video>

### Kubinec
<video autoplay loop muted controls width="100%">
  <source src="/assets/2021-07-25-sdt/comparison-kubinec.mp4" type="video/mp4">
</video>

### Luo
<video autoplay loop muted controls width="100%">
  <source src="/assets/2021-07-25-sdt/comparison-luo.mp4" type="video/mp4">
</video>

### Xing
<video autoplay loop muted controls width="100%">
  <source src="/assets/2021-07-25-sdt/comparison-xing.mp4" type="video/mp4">
</video>

## Video

<div class="video-container">
    <iframe class="video" allow="autoplay; encrypted-media" allowfullscreen
        src="https://www.youtube.com/embed/yu-5gUHn6h8">
    </iframe>
</div>

## Cite

```
@inproceedings{qian2021speech,
  title={Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates},
  author={Qian, Shenhan and Tu, Zhi and Zhi, Yihao and Liu, Wen and Gao, Shenghua},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages={11057--11066},
  year={2021},
  organization={IEEE}
}
```
